spark:
  app_name: main_mix_v2

pipeline:
  type: LanceWriter
  output_path: /home/dataset/kaiyuan/v2/stable_phase_v2
  select_cols: ["text_tokenized"]
  shuffle: true
  mode: overwrite
  child_configs:
  # chinese
  ## fineweb-edu-chinese-v2.1
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: FilterByRatio
        keep_ratio: 0.20
        # 430B * 0.20 = 86B tokens, ~90B
        column: "score"
        comparison: "larger"
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/cn_dataset/tokens/Fineweb_Edu_Chinese_dedup_cleaned
            select_cols: ["text_tokenized", "score"]
  ## finewiki-zh
  - type: ColumnSelect
    select_cols: ["text_tokenized"]
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/cn_dataset/tokens/finewiki/zh
  # ## undl_zh2en_aligned
  # - type: ColumnSelect
  #   select_cols: ["text_tokenized"]
  #   child_configs:
  #   - type: TokenCounter_v2
  #     input_col: "text_tokenized"
  #     drop_intermediate: True
  #     child_configs:
  #     - type: LanceReader
  #       input_path: /home/dataset/cn_dataset/tokens/undl_zh2en_aligned
  # ## baidu_baike
  # - type:  ColumnSelect
  #   select_cols: ["text_tokenized"] 
  #   child_configs:
  #   - type: TokenCounter_v2
  #     input_col: "text_tokenized"
  #     drop_intermediate: True
  #     child_configs:
  #     - type: LanceReader
  #       input_path: /home/dataset/cn_dataset/tokens/baidu_baike
  # english
  ## fineweb-edu-dedup, ~190B
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/en_dataset/tokens/fineweb-edu-dedup
  ## finepdf, ~100B
  # - type: ColumnSelect
  #   select_cols: ["text_tokenized"] 
  #   child_configs:
  #   - type: TokenCounter_v2
  #     input_col: "text_tokenized"
  #     drop_intermediate: True
  #     child_configs:
  #     - type: FilterByRatio
  #       keep_ratio: 0.08
  #       # 1100B * 0.08 = 88B tokens ~100B
  #       column: "fineweb-edu-classifier"
  #       comparison: "larger"
  #       child_configs:
  #       - type: TokenCounter_v2
  #         input_col: "text_tokenized"
  #         drop_intermediate: True
  #         child_configs:
  #         - type: LanceReader
  #           input_path: /home/dataset/en_dataset/tokens/eng_Latn_dedup_edu_ascend
  ## dclm
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: FilterByRatio
        keep_ratio: 0.40
        # 610B * 0.30 = 180B tokens ~ 150B
        column: "fasttext_openhermes_reddit_eli5_vs_rw_v2_bigram_200k_train_prob"
        comparison: "larger"
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/en_dataset/tokens/dclm_dedup
  ## dolmino/flan 29G
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: Selector
        selection_ratio: 0.1
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/en_dataset/tokens/dolmino-mix-1124/flan
  ## dolmino/pes2o 86G
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: Selector
        selection_ratio: 0.05
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/en_dataset/tokens/dolmino-mix-1124/pes2o
  ## wiki 12G
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/en_dataset/tokens/finewiki/en
  ## arxiv 32.5G
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/en_dataset/tokens/arxiv
  ## cosmopedia-v2 50G
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/en_dataset/tokens/cosmopedia-v2
  # math
  ## finemath
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/math_dataset/tokens/finemath
  ## open-web-math
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/math_dataset/tokens/open-web-math
  ## megamath-web-pro
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/math_dataset/tokens/MegaMath/megamath-web-pro
  # code
  ## stackexchange
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: LanceReader
        input_path: /home/dataset/code_dataset/tokens/stackexchange
  ## text-code
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: Selector
        selection_ratio: 0.5
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/math_dataset/tokens/MegaMath/megamath-text-code-block
  ## starcoder
  - type: ColumnSelect
    select_cols: ["text_tokenized"] 
    child_configs:
    - type: TokenCounter_v2
      input_col: "text_tokenized"
      drop_intermediate: True
      child_configs:
      - type: FilterByRatio
        keep_ratio: 0.10
        # 190B * 0.10 = 19B tokens
        column: "max_stars_count"
        comparison: "larger"
        child_configs:
        - type: TokenCounter_v2
          input_col: "text_tokenized"
          drop_intermediate: True
          child_configs:
          - type: LanceReader
            input_path: /home/dataset/code_dataset/tokens/starcoder_tokens
            select_cols: ["text_tokenized", "max_stars_count"]