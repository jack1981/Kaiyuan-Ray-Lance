spark:
  app_name: finewiki_en-tokens
pipeline:
  type: ParquetWriter
  shuffle: False
  mode: overwrite
  select_cols: ["text_tokenized"]
  output_path: /home/dataset/en_dataset/tokens/finewiki/en
  child_configs:
    - type: Tokenization
      input_col: "text"
      output_col: "text_tokenized"
      tokenizer_name_or_path: "/mnt/qwen2_tokenizer/tokenizer"
      child_configs:
        - type: ParquetReader
          input_path: /home/dataset/en_dataset/parquets/finewiki/en
