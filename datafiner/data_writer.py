"""Dataset sink nodes with Lance write modes and block-shape controls.

This module implements writer-side behaviors such as cache read-through,
write-mode mapping, and pre-write repartition caps.
In the report context, writer nodes correspond to root-level materialization of
mixed/filtered curricula generated by the pipeline tree.
See also `datafiner/data_reader.py` and `datafiner/dataset_utils.py`.
"""

from __future__ import annotations

from abc import ABC, abstractmethod

from datafiner.base import PipelineNode
from datafiner.dataset_utils import (
    cap_dataset_blocks_for_write,
    dataset_num_blocks,
    log_dataset_stats,
    normalize_lance_path,
    path_exists,
    select_columns,
    timed_stage,
    union_children,
)
from datafiner.register import register


class DataWriter(PipelineNode, ABC):
    """Abstract sink node that consumes child datasets and writes outputs.

    Inputs/outputs:
        Takes an output path and optional projection/shuffle controls; `run()`
        returns the dataset passed to/produced by `write`.

    Side effects:
        Subclasses perform filesystem/object-store writes.

    Assumptions:
        Writer nodes aggregate all children via positional union.
    """

    def __init__(
        self,
        runtime,
        output_path: str,
        shuffle: bool = True,
        select_cols: list = None,
        child_configs: list = None,
    ):
        """Initialize writer defaults shared by all sink implementations.

        Args:
            runtime: Shared runtime config.
            output_path: Target sink path.
            shuffle: Whether to randomize row order before writing.
            select_cols: Optional projection list before write.
            child_configs: Upstream node configs.

        Returns:
            None.

        Side effects:
            None.

        Assumptions:
            Shuffle uses Ray global random shuffle semantics.
        """
        super().__init__(runtime, child_configs)
        self.output_path = output_path
        self.shuffle = shuffle
        self.select_cols = select_cols

    @abstractmethod
    def write(self, ds):
        """Write a dataset to the sink and return a dataset object.

        Inputs/outputs:
            Accepts the prepared dataset and returns dataset exposed downstream.

        Side effects:
            Performs sink-specific I/O.

        Assumptions:
            Subclasses preserve dataset schema unless explicitly documented.
        """
        pass

    def run(self):
        """Collect child data, apply common pre-write transforms, then write.

        Inputs/outputs:
            No inputs; returns result from `write(ds)`.

        Side effects:
            Executes child nodes, optional shuffle/projection, and sink I/O.

        Assumptions:
            Child datasets are union-compatible by position.
        """
        with timed_stage(self.runtime, f"writer.input:{self.__class__.__name__}"):
            ds = union_children(self.children, by_name=False)
        if self.select_cols:
            ds = select_columns(ds, self.select_cols, runtime=self.runtime)
        if self.shuffle:
            ds = ds.random_shuffle()
        log_dataset_stats(self.runtime, ds, f"writer.pre_write:{self.__class__.__name__}")
        return self.write(ds)


@register("LanceWriter")
class LanceWriter(DataWriter):
    """Write datasets to Lance with Spark-like mode semantics.

    Inputs/outputs:
        Consumes child dataset(s) and writes to Lance path using mode mappings.

    Side effects:
        Reads existing Lance data for cache modes and writes Lance output files.

    Assumptions:
        `mode` values map to Ray's supported Lance write modes.
    """

    def __init__(
        self,
        runtime,
        output_path: str,
        shuffle: bool = False,
        num_output_files: int = None,
        num_read_partitions: int = None,
        mode: str = "overwrite",
        select_cols: list = None,
        child_configs: list = None,
        storage_options: dict | None = None,
    ):
        """Initialize Lance writer options.

        Args:
            runtime: Shared runtime config.
            output_path: Destination Lance path.
            shuffle: Whether to shuffle rows before write.
            num_output_files: Optional explicit output block count.
            num_read_partitions: Optional repartition count when reading cache.
            mode: Write mode (`overwrite`, `append`, `ignore`, `create`,
                `read_if_exists`).
            select_cols: Optional projection list before write.
            child_configs: Upstream node configs.
            storage_options: Optional storage options override.

        Returns:
            None.

        Side effects:
            None during initialization.

        Assumptions:
            Storage options default to runtime-level options when omitted.
        """
        super().__init__(runtime, output_path, shuffle, select_cols, child_configs)
        self.num_output_files = num_output_files
        self.num_read_partitions = num_read_partitions
        self.mode = mode
        self.storage_options = storage_options or self.runtime.storage_options

    def _read_existing(self):
        """Attempt to read existing output dataset for cache-hit modes.

        Inputs/outputs:
            Reads `self.output_path` and returns dataset or `None`.

        Side effects:
            Performs Lance read I/O and optional repartition.

        Assumptions:
            Any read failure means cache miss and should not raise.
        """
        ds = None
        try:
            ds = __import__("ray").data.read_lance(
                normalize_lance_path(self.output_path),
                storage_options=self.storage_options,
            )
        except Exception:
            ds = None

        if ds is None:
            return None

        if self.num_read_partitions is not None and self.num_read_partitions > 0:
            ds = ds.repartition(self.num_read_partitions, shuffle=False)
        return ds

    def run(self):
        """Handle cache semantics, then execute standard writer preparation flow.

        Inputs/outputs:
            No inputs; returns cached or freshly written dataset.

        Side effects:
            May read existing Lance data and/or write new output.

        Assumptions:
            `read_if_exists` should never overwrite when cache is readable.
        """
        if self.mode == "read_if_exists":
            print(
                f"[LanceWriter] Mode 'read_if_exists' set. Checking cache: {self.output_path}"
            )
            existing = self._read_existing()
            if existing is not None:
                print("[LanceWriter] Cache hit. Reading from path.")
                return existing
            print("[LanceWriter] Cache miss. Proceeding to compute and write.")

        print("[LanceWriter] Computing dataset from children...")
        with timed_stage(self.runtime, f"writer.input:{self.__class__.__name__}"):
            ds = union_children(self.children, by_name=False)
        if self.select_cols:
            ds = select_columns(ds, self.select_cols, runtime=self.runtime)
        if self.shuffle:
            ds = ds.random_shuffle()
        log_dataset_stats(self.runtime, ds, f"writer.pre_write:{self.__class__.__name__}")
        return self.write(ds)

    def write(self, ds):
        """Persist a dataset to Lance with optional repartition and mode mapping.

        Args:
            ds: Prepared dataset ready for sink write.

        Returns:
            Dataset object used for writing.

        Side effects:
            May repartition dataset and writes Lance files.

        Assumptions:
            Without explicit `num_output_files`, block capping avoids tiny-file
            explosion in large fan-out plans.
        """
        if self.num_output_files is not None and self.num_output_files > 0:
            print(
                f"[LanceWriter] Repartitioning dataset to {self.num_output_files} blocks for write."
            )
            ds = ds.repartition(self.num_output_files, shuffle=False)
        else:
            ds = cap_dataset_blocks_for_write(ds, self.runtime)

        effective_mode = self.mode
        if self.mode == "read_if_exists":
            effective_mode = "overwrite"

        if effective_mode == "ignore" and path_exists(self.output_path):
            print(f"[LanceWriter] Mode 'ignore': target exists, skipping write {self.output_path}")
            return ds

        ray_mode = {
            "overwrite": "overwrite",
            "append": "append",
            "ignore": "create",
            "create": "create",
        }.get(effective_mode, "create")

        print(f"[LanceWriter] Writing data to {self.output_path} (mode={ray_mode})")
        # NOTE(readability): We map Spark-like mode names to Ray Lance writer
        # modes so existing YAML configs keep their previous semantics.
        with timed_stage(self.runtime, f"writer.write_lance:{self.output_path}"):
            ds.write_lance(
                normalize_lance_path(self.output_path),
                mode=ray_mode,
                storage_options=self.storage_options,
            )
        log_dataset_stats(self.runtime, ds, f"writer.output:{self.__class__.__name__}")
        return ds


@register("LanceWriterZstd")
class LanceWriterZstd(LanceWriter):
    """
    Lance writer with extra repartition controls.
    """

    def __init__(
        self,
        runtime,
        output_path: str,
        shuffle: bool = False,
        num_output_files: int = None,
        num_read_partitions: int = None,
        mode: str = "overwrite",
        select_cols: list = None,
        child_configs: list = None,
        compression_level: int = 9,
        use_coalesce: bool = False,
        merge_count: int = 128,
        storage_options: dict | None = None,
    ):
        """Initialize Lance writer variant with extra coalescing controls.

        Args:
            runtime: Shared runtime config.
            output_path: Destination Lance path.
            shuffle: Whether to shuffle rows before write.
            num_output_files: Optional target output block count.
            num_read_partitions: Optional cache-read repartition count.
            mode: Write mode.
            select_cols: Optional projection list before write.
            child_configs: Upstream node configs.
            compression_level: Legacy compatibility knob retained for API parity.
            use_coalesce: Whether to reduce blocks before writing.
            merge_count: Block merge ratio used when coalescing without explicit
                output block count.
            storage_options: Optional storage options override.

        Returns:
            None.

        Side effects:
            None during initialization.

        Assumptions:
            Compression-level knob is retained even though Ray Lance writer owns
            actual encoding internals.
        """
        super().__init__(
            runtime,
            output_path,
            shuffle,
            num_output_files,
            num_read_partitions,
            mode,
            select_cols,
            child_configs,
            storage_options,
        )
        self.compression_level = compression_level
        self.use_coalesce = use_coalesce
        self.merge_count = merge_count

    def write(self, ds):
        """Optionally coalesce blocks, then delegate to base Lance writer.

        Args:
            ds: Dataset prepared for write.

        Returns:
            Dataset returned by parent `write`.

        Side effects:
            May materialize/repartition dataset before writing.

        Assumptions:
            Repartition with `shuffle=False` best approximates Spark coalesce.
        """
        # NOTE(readability): Ray Data does not expose Spark-style coalesce, so
        # we emulate it with deterministic non-shuffle repartitioning.
        if self.use_coalesce and self.num_output_files is not None and self.num_output_files > 0:
            ds = ds.repartition(self.num_output_files, shuffle=False)
        elif self.use_coalesce:
            num_blocks, ds = dataset_num_blocks(ds, materialize_if_needed=True)
            if num_blocks is not None:
                target = max(1, int(num_blocks / max(self.merge_count, 1)))
                ds = ds.repartition(target, shuffle=False)

        return super().write(ds)
