spark:
  app_name: finemath_tokens
pipeline:
  type: ParquetWriter
  shuffle: False
  mode: overwrite
  select_cols: ["text_tokenized", "token_count", "char_count", "url", "score"]
  output_path: /home/dataset/math_dataset/tokens/finemath
  child_configs:
    - type: Tokenization
      input_col: "text"
      output_col: "text_tokenized"
      tokenizer_name_or_path: "/mnt/qwen2_tokenizer/tokenizer"
      child_configs:
        - type: ParquetReader
          input_path: /home/dataset/math_dataset/parquets/finemath/finemath-4plus/
