from datafiner.base import PipelineNode
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from datafiner.register import register


@register("TokenCounter_v2")
class TokenCounter_v2(PipelineNode):
    """
    Counts the number of tokens in a pre-tokenized column.

    This class assumes that the input DataFrame has a column containing
    arrays of token IDs, typically generated by a tokenizer.
    """

    def __init__(
        self,
        spark: SparkSession,
        child_configs: list = None,
        input_col: str = "text_tokenized",  # Default to the tokenized column
        count_col: str = "token_count",
        with_duplicate_count: bool = False,
        duplicate_count_col: str = "duplicate_count",
        summary: bool = True,
        drop_intermediate: bool = False,
        task_name: str = None,
    ):
        super().__init__(spark, child_configs)
        self.input_col = input_col
        self.with_duplicate_count = with_duplicate_count
        self.duplicate_count_col = duplicate_count_col
        self.summary = summary
        self.drop_intermediate = drop_intermediate
        self.count_col = count_col
        self.task_name = task_name

    def run(self):
        df = self.children[0].run()
        if len(self.children) > 1:
            for child in self.children[1:]:
                df = df.union(child.run())

        # Count tokens for each row using the native size() function
        # This is the most efficient way to get the length of an array column.
        df = df.withColumn(self.count_col, F.size(F.col(self.input_col)))

        if self.with_duplicate_count:
            df = df.withColumn(
                self.count_col, F.col(self.duplicate_count_col) * F.col(self.count_col)
            )

        if self.summary:
            # Calculate and print the sum of all token_count values
            total_tokens = df.agg(F.sum(self.count_col)).collect()[0][0]
            if self.task_name:
                print(f"Task: {self.task_name}")
            print(f"Total token count: {total_tokens}")

            # Also show some statistics
            if self.with_duplicate_count:
                row_count = df.agg(F.sum(self.duplicate_count_col)).collect()[0][0]
            else:
                row_count = df.count()

            avg_tokens = total_tokens / row_count if row_count > 0 else 0
            print(f"Total rows (or documents): {row_count}")
            print(f"Average tokens per row: {avg_tokens:.2f}")

        if self.drop_intermediate:
            # Drop the intermediate token_count column before returning
            df = df.drop(self.count_col)
        return df
